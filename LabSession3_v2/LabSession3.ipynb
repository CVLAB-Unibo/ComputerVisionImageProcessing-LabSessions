{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Images and Videos\n",
    "## Computer Vision and Image Processing - Lab Session 3\n",
    "### Prof: Luigi Di Stefano, luigi.distefano@unibo.it\n",
    "### Tutor: Pierluigi Zama Ramirez, pierluigi.zama@unibo.it - Alex Costanzino, alex.costanzino2@unibo.it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Images\n",
    "\n",
    "## Histogram\n",
    "\n",
    "Point Operators are image processing operators aimed at enhancing the quality of the input image(e.g. the contrast).\n",
    "Most of such operators rely on the computation of the gray-level histogram (intensity histogram) of the input image.\n",
    "Loaded a **grayscale** image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"LabSession3Images/image.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute its gray-level histogram using **np.histogram(flat_image, bins=256, range=[0,256])**\n",
    "\n",
    "Where:\n",
    "* **flat_image**: image flattened to be a 1D array\n",
    "* **bins**: bounds of the bins. If bins is an integer, the bounds are all numbers from minimum to maximum (exluded). Otherwise, they can be also a custom sequence.\n",
    "* **range**: minimum and maximum values of our histograms.\n",
    "\n",
    "Bins will have 257 elements, because Numpy calculates bins as 0-0.99, 1-1.99, 2-2.99 etc. So final range would be 255-255.99. The last value 256 is the last edge of the last bin but it is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(img.flatten(), 256, [0,256])\n",
    "print(\"Histogram shape: \", hist.shape)\n",
    "print(\"Bins shape: \", bins.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where __hist[i]__ are the number of pixels with the gray-scale value $i$. Bins are the indexes of each bin (i.e. numbers from 0 to 255).\n",
    "\n",
    "Let us see how many pixels have the value 20 in our image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_value = 20\n",
    "print(hist[grayscale_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stem(hist, use_line_collection=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Point Operator: Linear Contrast Stretching\n",
    "\n",
    "An image featuring a small gray-level range has likely poor contrast. The contrast can be enhanced by linearly stretching the intensities to span a larger interval (typically, the full available range).\n",
    "\n",
    "Linear stretching at the full available range:\n",
    "\n",
    "<img src=\"LabSession3Images/linear_stretching_plot.png\" width=\"320\">\n",
    "<img src=\"LabSession3Images/linear_stretching.png\" width=\"480\">\n",
    "\n",
    "Should most pixel lie in a small interval while there exist a few dark and bright outliers, the linear function would approximate the identity and thus be ineffective. In such a case, $r_{min}$ and $r_{max}$ can be taken equal to some percentiles of the distribution (e.g. 5% and 95%), with smaller and higher input intensities mapped to 0 and 255 respectively.\n",
    "\n",
    "## Local Operators: Convolutions and Correlations \n",
    "\n",
    "Local Operators compute the new intensity of a pixel, p, based on the intensities of those belonging to a neighbourhood of p.\n",
    "\n",
    "<img src=\"LabSession3Images/local_op.png\" width=\"320\">\n",
    "\n",
    "An important sub-class is given by the so called **Linear Shift-Invariant** (LSI) operators.\n",
    "LSI operators are defined **convolving** a kernel with an image.\n",
    "\n",
    "However, we know that **convolutions and correlations** are the same if we have a **symmetric** kernel respect to the **origin**:\n",
    "\n",
    "<img src=\"LabSession3Images/conv_corr.png\" width=\"128\">\n",
    "\n",
    "Practically, local operator kernels are usually symmetric about the origin so usually we compute **correlations** instead of convolutions because they are **easier to implement**. In case we have not symmetric filter respect to the origin we can still use correlations but we need to first **flip the kernel**.\n",
    "\n",
    "Given an image and a kernel (aka filter) in OpenCV we can perform a correlation with the following command:\n",
    "\n",
    "**cv2.filter2D(image, -1, kernel, anchor)**\n",
    "\n",
    "where **anchor** of the kernel indicates the relative position of a filtered point within the kernel; the anchor should lie within the kernel; default value (-1,-1) means that the anchor is at the kernel center.\n",
    "\n",
    "If we need to compute a convolution we need to flip the kernel using:\n",
    "\n",
    "**cv2.flip(kernel)**\n",
    "\n",
    "In case of flip we need to set the new anchor to (kernel.cols - anchor.x - 1, kernel.rows - anchor.y - 1).\n",
    "\n",
    "## Local Operators: Some Peculiar Filters\n",
    "\n",
    "### Gaussian filter\n",
    "Gaussian filter is a special case of LSI operator where we sample from a 2D gaussian the values of our kernel.  \n",
    "\n",
    "<img src=\"LabSession3Images/gaussian.png\" width=\"320\">\n",
    "\n",
    "Given standard deviation $\\sigma$ and kernel size we can obtain a 1D gaussian kernel in OpenCV with:\n",
    "\n",
    "**gk = cv2.getGaussianKernel(kernel_size,sigma)**\n",
    "\n",
    "which 1D having shape $(KernelSize , 1)$\n",
    "\n",
    "If we want a 2D gaussian filter with both $\\sigma_x$ and $\\sigma_y$ having the same value we can multiply the two 1D kernels with:\n",
    "\n",
    "**gk_2D = gk.dot(gk.transpose())**\n",
    "\n",
    "Otherwise, we can obtain the same result applying first the 1D gaussian kernel **gk** and then, applying its tranposed one **gk$^T$**. Applying twice a 1D kernel instead of a 2D filter can be faster, expecially in case of big kernels.\n",
    "\n",
    "We can also use an high level API of OpenCV which given the kernel size and sigmas compute the gaussian filtering:\n",
    "\n",
    "**gauss_out = cv2.GaussianBlur(image, (k_size,k_size) , sigma)**\n",
    "\n",
    "### Bilateral filter\n",
    "\n",
    "<img src=\"LabSession3Images/bilateral.png\" width=\"512\">\n",
    "\n",
    "Bilateral filter is highly effective at **noise removal** while **preserving edges**. \n",
    "\n",
    "The operation is **slower** compared to other filters. \n",
    "\n",
    "**Gaussian filters** is a function of **space** alone and does not consider whether pixels have almost the same intensity value or not resulting in blurred edges. \n",
    "\n",
    "On the other hand, the **bilateral filter** also uses a **Gaussian filter** in the **space** domain, but it also uses one more (multiplicative) **Gaussian filter** component which is a function of **pixel intensity differences**. \n",
    "\n",
    "The Gaussian function of space makes sure that only pixels are ‘spatial neighbors’ are considered for filtering, while the Gaussian component applied in the intensity domain (a Gaussian function of intensity differences) ensures that only those pixels with intensities similar to that of the central pixel (‘intensity neighbors’) are included to compute the blurred intensity value. As a result, this method preserves edges, since for pixels lying near edges, neighboring pixels placed on the other side of the edge, and therefore exhibiting large intensity variations when compared to the central pixel, will not be included for blurring.\n",
    "\n",
    "We can apply a bilateral filter in OpenCV with:\n",
    "\n",
    "**cv2.bilateralFilter(image, filter_size, sigmaColor, sigmaSpace)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Videos\n",
    "\n",
    "A video is a temporal sequence of images, namely **Frames**.\n",
    "\n",
    "<img src=\"LabSession3Images/Eadward-Muybridge-Horse-in-Motion.jpg\" width=\"512\">\n",
    "\n",
    "The number of frames in 1 second of video is called **frame rate**\n",
    "\n",
    "In a video we can elaborate each frame separately with processing algorithms or we can elaborate only keyframe of the original video and the missing ones are reconstructed by interpolation.\n",
    "\n",
    "## Load a Video from File\n",
    "\n",
    "To load a video in OpenCV you need to create a VideoCapture object. Its argument can be either the device index or the name of a video file.\n",
    "A device index is just the number to specify which camera. Normally one camera will be connected (as in my case). So I simply pass 0 (or -1).\n",
    "You can select the second camera by passing 1 and so on. After that, you can capture frame-by-frame. But at the end, don't forget to release the capture.\n",
    "\n",
    "Let us now try to load a video from a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('LabSession3Images/video.avi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, cap may not have initialized the capture. In that case, this code shows an error. You can check whether it is initialized or not by the method cap.isOpened(). If it is True, OK. Otherwise open it using cap.open().\n",
    "\n",
    "In case the capture is open we can get a frame of the capture in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "print(ret, frame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where _ret_ is a boolean variable which is True if we read the frame correctly and _frame_ is an image. Let us visualize the captured frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ret and not frame is None:\n",
    "    # Disabling matplotlib axis for better visualization\n",
    "    plt.axis('off')\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also access some of the features of this video using cap.get(propId) method where propId is a number from 0 to 18. Each number denotes a property of the video (if it is applicable to that video). Some of these values can be modified using cap.set(propId, value).\n",
    "\n",
    "For example to get the width and height of the frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Width: \" , cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print(\"Height: \", cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My video is of resolution 320x180.\n",
    "\n",
    "To release the capture stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to play our video in **jupyter notebook** we can do the following (a bit slow because of jupyter not ideal for playing videos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional library to properply play videos on jupyter notebook\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Put the code in try-except statements catch the keyboard exception and release the camera device and \n",
    "# continue with the rest of code.\n",
    "def play_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    # Try-except statement to manage exceptions\n",
    "    try:\n",
    "        while(True):\n",
    "            # Capture frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or frame is None:\n",
    "                # Release the Video if ret is false\n",
    "                cap.release()\n",
    "                print(\"Released Video Resource\")\n",
    "                # Break exit the for loops\n",
    "                break\n",
    "            \n",
    "            # Display the frame\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            plt.axis('off')\n",
    "            plt.title(\"Input Stream\")\n",
    "            plt.imshow(frame)\n",
    "            plt.show()\n",
    "            \n",
    "            # Clear cell output when new frame is available\n",
    "            clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        # If we press stop (jupyter GUI) release the video\n",
    "        cap.release()\n",
    "        print(\"Released Video Resource\")\n",
    "\n",
    "play_video('LabSession3Images/video.avi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__N.B__: It can happen that we want to play a video from a non-jupyter enviroment. In that case we can modify the above code as follows:\n",
    "\n",
    "```python\n",
    "def play_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret or frame is None:\n",
    "            # Release the Video if ret is false\n",
    "            cap.release()\n",
    "            print(\"Released Video Resource\")\n",
    "            break\n",
    "\n",
    "        # !!! DISPLAYING CHANGE RESPECT TO JUPYTER VERSION !!!\n",
    "        # Displaying with OpenCV (Not working in Jupyter)\n",
    "        cv2.imshow('frame', frame)\n",
    "        # Stop playing when entered 'q' from keyboard\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Video from a Camera (Only if Camera Available)\n",
    "If we want to load a video from a camera we can open our video capture giving a device id as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working only if a camera is available.\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Width: \" , cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print(\"Height: \", cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resolution is 640x480 by default. We can change it using cap.set (not all resolutions are possible depending on the camera driver):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now frames have the desired resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "print(ret, frame.shape)\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to reproduce our webcam stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Video\n",
    "\n",
    "To save a video on a file we need to create a VideoWriter object specifying:\n",
    "* **Filename** (eg: output.avi)\n",
    "* **FourCC code**: 4-byte code used to specify the video codec. The list of available codes can be found in fourcc.org. It is platform dependent. In Windows DIVX is the preferred choice while on linux we have several standards such as DIVX, XVID, X264 etc..\n",
    "* **FPS**: number of frames per second\n",
    "* **Frame size**\n",
    "* Flag **isColor**: If it is True, the encoder expect color frame, otherwise it works with grayscale frame.\n",
    "\n",
    "Let us try to load and save a video with each frame flipped along vertical axis. Load the original video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Video\n",
    "cap = cv2.VideoCapture(\"LabSession3Images/video.avi\")\n",
    "\n",
    "# Getting original video params\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the VideoWriter based on the parameters of the original video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "# N.B. we need to specify the correct width and height of the frames otherwise we will not be able to reproduce the video\n",
    "out = cv2.VideoWriter('output.avi', fourcc, fps, (w,  h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flip and save frame by frame the video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or frame is None:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "    frame = np.flip(frame, axis=0)\n",
    "    # write the flipped frame\n",
    "    out.write(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Release the resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the flipped video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video('output.avi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
