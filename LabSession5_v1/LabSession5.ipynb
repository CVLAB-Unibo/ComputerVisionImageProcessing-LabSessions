{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Streams\n",
    "## Computer Vision and Image Processing - Lab Session 5\n",
    "### Prof: Luigi Di Stefano, luigi.distefano@unibo.it\n",
    "### Tutor: Pierluigi Zama Ramirez, pierluigi.zama@unibo.it - Riccardo Spezialetti, riccardo.spezialetti@unibo.it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import additional library to properply play videos on jupyter notebook\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A video is a temporal sequence of images, namely **Frames**.\n",
    "\n",
    "<img src=\"LabSession5Images/Eadward-Muybridge-Horse-in-Motion.jpg\" width=\"512\">\n",
    "\n",
    "The number of frames in 1 second of video is called **frame rate**\n",
    "\n",
    "In a video we can elaborate each frame separately with processing algorithms or we can elaborate only keyframe of the original video and the missing ones are reconstructed by interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Video from File\n",
    "\n",
    "To load a video in OpenCV you need to create a VideoCapture object. Its argument can be either the device index or the name of a video file.\n",
    "A device index is just the number to specify which camera. Normally one camera will be connected (as in my case). So I simply pass 0 (or -1).\n",
    "You can select the second camera by passing 1 and so on. After that, you can capture frame-by-frame. But at the end, don't forget to release the capture.\n",
    "\n",
    "Let us now try to load a video from a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('LabSession5Images/video.avi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, cap may not have initialized the capture. In that case, this code shows an error. You can check whether it is initialized or not by the method cap.isOpened(). If it is True, OK. Otherwise open it using cap.open().\n",
    "\n",
    "In case the capture is open we can get a frame of the capture in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "print(ret, frame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where _ret_ is a boolean variable which is True if we read the frame correctly and _frame_ is an image. Let us visualize the captured frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ret and not frame is None:\n",
    "    # Disabling matplotlib axis for better visualization\n",
    "    plt.axis('off')\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also access some of the features of this video using cap.get(propId) method where propId is a number from 0 to 18. Each number denotes a property of the video (if it is applicable to that video). Some of these values can be modified using cap.set(propId, value).\n",
    "\n",
    "For example to get the width and height of the frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Width: \" , cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print(\"Height: \", cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My video is of resolution 320x180.\n",
    "\n",
    "To release the capture stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to play our video in **jupyter notebook** we can do the following (a bit slow because of jupyter not ideal for playing videos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the code in try-except statements catch the keyboard exception and release the camera device and \n",
    "# continue with the rest of code.\n",
    "def play_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    # Try-except statement to manage exceptions\n",
    "    try:\n",
    "        while(True):\n",
    "            # Capture frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or frame is None:\n",
    "                # Release the Video if ret is false\n",
    "                cap.release()\n",
    "                print(\"Released Video Resource\")\n",
    "                # Break exit the for loops\n",
    "                break\n",
    "            \n",
    "            # Display the frame\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            plt.axis('off')\n",
    "            plt.title(\"Input Stream\")\n",
    "            plt.imshow(frame)\n",
    "            plt.show()\n",
    "            \n",
    "            # Clear cell output when new frame is available\n",
    "            clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        # If we press stop (jupyter GUI) release the video\n",
    "        cap.release()\n",
    "        print(\"Released Video Resource\")\n",
    "\n",
    "play_video('LabSession5Images/video.avi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__N.B__: It can happen that we want to play a video from a non-jupyter enviroment. In that case we can modify the above code as follows:\n",
    "\n",
    "```python\n",
    "def play_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret or frame is None:\n",
    "            # Release the Video if ret is false\n",
    "            cap.release()\n",
    "            print(\"Released Video Resource\")\n",
    "            break\n",
    "\n",
    "        # !!! DISPLAYING CHANGE RESPECT TO JUPYTER VERSION !!!\n",
    "        # Displaying with OpenCV (Not working in Jupyter)\n",
    "        cv2.imshow('frame', frame)\n",
    "        # Stop playing when entered 'q' from keyboard\n",
    "        if cv.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Video from a Camera (Only if Camera Available)\n",
    "If we want to load a video from a camera we can open our video capture giving a device id as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working only if a camera is available.\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Width: \" , cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print(\"Height: \", cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resolution is 640x480 by default. We can change it using cap.set (not all resolutions are possible depending on the camera driver):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now frames have the desired resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "print(ret, frame.shape)\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to reproduce our webcam stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Video\n",
    "\n",
    "To save a video on a file we need to create a VideoWriter object specifying:\n",
    "* **Filename** (eg: output.avi)\n",
    "* **FourCC code**: 4-byte code used to specify the video codec. The list of available codes can be found in fourcc.org. It is platform dependent. In Windows DIVX is the preferred choice while on linux we have several standards such as DIVX, XVID, X264 etc..\n",
    "* **FPS**: number of frames per second\n",
    "* **Frame size**\n",
    "* Flag **isColor**: If it is True, the encoder expect color frame, otherwise it works with grayscale frame.\n",
    "\n",
    "Let us try to load and save a video with each frame flipped along vertical axis. Load the original video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Video\n",
    "cap = cv2.VideoCapture(\"LabSession5Images/video.avi\")\n",
    "\n",
    "# Getting original video params\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the VideoWriter based on the parameters of the original video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "# N.B. we need to specify the correct width and height of the frames otherwise we will not be able to reproduce the video\n",
    "out = cv2.VideoWriter('output.avi', fourcc, fps, (w,  h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flip and save frame by frame the video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or frame is None:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "    frame = np.flip(frame, axis=0)\n",
    "    # write the flipped frame\n",
    "    out.write(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Release the resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the flipped video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video('output.avi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Detection Algorithms\n",
    "\n",
    "Change Detection: detection of “meaningful” changes occurring in a scene by processing of images captured at different time instants.\n",
    "* Input: two (at least!) or more images of the monitored scene.\n",
    "* Output: binary image, called \"change mask“: each pixel is assigned one between two values (labels) c, u (\"changed\",  unchanged\"): c if meaningful changes occur at the pixel, u otherwise (commonly, c = 255, u = 0 → white/ black).\n",
    "\n",
    "<img src=\"LabSession5Images/ChangeDetection.png\" width=\"512\">\n",
    "\n",
    "Example of Applications are Traffic monitoring, Video compression, Security etc ..\n",
    "\n",
    "The following Change Detection Algorithms we will talk later are based on the following **working assumptions**:\n",
    "* **Static camera**\n",
    "* **High frame rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance functions between two images\n",
    "\n",
    "We define as $F_t(i,j) \\in R^2$ our grayscale frame and as $F_t(i,j) \\in R^n$ our colored frame where $n$ are the number of channels, $i,j$ are the pixel coordinates and $t$ the temporal position in the video sequence. We define $d(F_t(i,j), F_{t-k}(i,j))$ as a distance function between two different frames where $k=0,1..,t$.\n",
    "\n",
    "Typical distance functions are the Holder norms of the difference vector. Given a vector $v\\in R^n$ and a norm $p$ they are defined as follows:\n",
    "\n",
    "$||v||_p =\\sqrt[p]{\\sum_{i=1}^n{|v_i|^p}}$\n",
    "\n",
    "With two frames with $n$ channels $F_t(i,j) = [f_1^t(i,j),…, f_n^t(i,j)]$ and $F_k(i,j) = [f_1^k(i,j),…, f_n^k(i,j)]$ the norms of the difference are defined as follows:\n",
    "\n",
    "$||F_t(i,j) - F_k(i,j)||_p = \\sqrt[p]{|f_1^t(i,j) - f_1^k(i,j)|^p  + ... + |f_n^t(i,j) - f_n^k(i,j)|^p}$\n",
    "\n",
    "Most used distance functions are:\n",
    "\n",
    "* Manhattan distance:  $L_1(i,j)=||F_t(i,j) - F_k(i,j)||_1 = \\sum_{i=1}^n{|f_i^t(i,j) - f_i^k(i,j)|}$\n",
    "* Euclidean distance: $L_2(i,j)=||F_t(i,j) - F_k(i,j)||_2 = \\sqrt{\\sum_{i=1}^n{|f_i^t(i,j) - f_i^k(i,j)|^2}}$\n",
    "* Maximum distance: $L_{\\infty}(i,j)=||F_t(i,j) - F_k(i,j)||_{\\infty} = max_i(|f_i^t(i,j) - f_i^k(i,j)|), i=1..n$ \n",
    "\n",
    "In case of colored images we usually have $n=3$ (the number of channels R,G,B).\n",
    "\n",
    "In case of grayscale image with $n=1$ the norms becomes simply the absolute value of the pixel-wise difference $|F_t(i,j) - F_k(i,j)|$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Two Frame Difference\n",
    "\n",
    "A generic two frame difference algorithm is defined in the following way:\n",
    "\n",
    "$C(i,j) = \\begin{cases} 255, & \\mbox{if } d(F_t(i,j), F_{t-l}(i,j))>T \\\\ 0, & otherwise \\end{cases}$\n",
    "\n",
    "Example in case of grayscale image:\n",
    "\n",
    "<img src=\"LabSession5Images/2FD.png\" width=\"768\">\n",
    "\n",
    "Example in case of colored image and $L_{\\infty}$:\n",
    "\n",
    "<img src=\"LabSession5Images/2FD_rgb.png\" width=\"768\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three Frame Difference\n",
    "\n",
    "The Three Frame Difference is a simple extension of the Two Frame Difference. Given three frames $F_t(i,j)$, $F_{t-1}(i,j)$, $F_{t-2}(i,j)$ we perform first the two-frame differences between frames $t$ and $t-1$ and between frames $t-1$ and $t-2$. Finally, we compute the intersection (binary AND) between the two obtained change masks.\n",
    "\n",
    "$C(i,j) = \\begin{cases} 255, & \\mbox{if } (d(F_t(i,j), F_{t-l}(i,j))>T) \\wedge (d(F_{t-1}(i,j), F_{t-2}(i,j))>T) \\\\ 0, & otherwise \\end{cases}$\n",
    "\n",
    "\n",
    "Example in case of colored image:\n",
    "\n",
    "<img src=\"LabSession5Images/3FD.png\" width=\"768\">\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Subtraction\n",
    "\n",
    "Very similar to the two-frame difference: the current frame is compared with a “model” (e.g. an image) of the background of the monitored scene (instead of the previous frame). Defined the background image as $B_t(i,j)$ we compute a general Background Subtraction algorithm in the following way:\n",
    "\n",
    "$C(i,j) = \\begin{cases} 255, & \\mbox{if } d(F_t(i,j), B_t(i,j))>T \\\\ 0, & otherwise \\end{cases}$\n",
    "\n",
    "Example with grayscale images:\n",
    "\n",
    "<img src=\"LabSession5Images/BGS.png\" width=\"768\">\n",
    "\n",
    "Example with colored images:\n",
    "\n",
    "<img src=\"LabSession5Images/BGS_rgb.png\" width=\"768\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
