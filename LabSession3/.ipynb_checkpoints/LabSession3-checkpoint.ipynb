{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Calibration\n",
    "## Computer Vision and Image Processing - Lab Session 3\n",
    "### Prof: Luigi Di Stefano, luigi.distefano@unibo.it\n",
    "### Tutor: Pierluigi Zama Ramirez, pierluigi.zama@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Camera calibration** is the process whereby all **parameters** defining the camera model are estimated for a specific camera device.\n",
    "\n",
    "The **pinhole camera model** is represented by the **Perspective Projection Matrix (PPM)**, which in turn can be decomposed into 3 independent tokens: **intrinsic parameter matrix ($A$), rotation matrix ($R$) and translation vector ($T$)**. Depending on the application, either the PPM only or also its independent components ($A, R, T$) need to be estimated.\n",
    "\n",
    "Many camera calibration algorithms do exist. The basic process, though, relies always on setting up a linear system of equations given a set of **known 3D-2D correspondences**, so as to then solve for the unknown camera parameters.\n",
    "\n",
    "To obtain the required correspondences specific physical objects (referred to as **calibration targets**) having easily detectable features (such as e.g. **chessboard** or dot patterns) are typically deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinhole camera model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general form of the Perspective Projection Matrix  (PPM or P) can be thought of as encoding the position of the camera wrt the world into G, the perspective projection carried out by a pinhole camera into the canonical $PPM [I|0]$ and, finally, the actual characteristics of the sensing device into $A$.\n",
    "\n",
    "<p style=\"text-align: center\"> $A[I│0]G$ or $P=A[R│T]$ </p>\n",
    "\n",
    "<img src=\"LabSession3Images/ppm.png\" width=\"256\" height=\"128\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrisics Parameters\n",
    "\n",
    "Matrix A, which models the characteristics of the image sensing device, is called **Intrinsic Parameter Matrix**. \n",
    "\n",
    "<img src=\"LabSession3Images/intrisics.png\" width=256>\n",
    "\n",
    "where:\n",
    "\n",
    "* $f$ is the **focal length** of the pinhole system\n",
    "* $k_u = \\frac{1}{\\delta u}$ , $ k_v = \\frac{1}{\\delta v} $ are, respectively, the inverse of the **horizontal and vertical pixel size**\n",
    "* $u_0$, $v_0$ are the coordinates of the **piercing point** wrt the top-left corner\n",
    "\n",
    "**Intrinsic parameters are 5** but can be **reduced** in number by setting $a_u = f*k_u$, $ a_v = f*k_v$ such quantities representing, respectively, the focal length expressed in horizontal and vertical pixel size. \n",
    "\n",
    "**The number of intrinsic parameters estimated by OpenCV is thus 4**. The pixel size is usually provided in the camera datasheet: if it is known, the metric focal length can be recovered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrinsics Parameters\n",
    "\n",
    "Matrix G, which encodes the position and orientation of the camera withresect to the WRF, is called Extrinsic Parameter Matrix.\n",
    "\n",
    "* As a **rotation matrix** **(3x3=9 entries)** has indeed only **3 independent parameters (DOF)**, which correspond to the **rotation angles** around the axis of the RF, the **total number of extrinsic parameter is 6** (**3 translation parameters, 3 rotation parameters**)\n",
    "\n",
    "* Hence, the general form of the PPM can be thought of as encoding the position of the camera wrt the world into G, the perspective projection carried out by a pinhole camera into the canonical $PPM[I|0]$ and, finally, the actual characteristics of the sensing device into A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Reference Frame\n",
    "\n",
    "The **World Reference Frame**, to ease the derivation and implementation of calibration routines, is assumed **attached with the chessboard**. Therefore, **different calibration images lead to different World Reference Frames**.\n",
    "In other words, you can actually doing your calibration by **moving the chessboard** (as depicted on the left picture), but the software sees those transformations as if you were **moving the camera** (as depicted on the right picture).\n",
    "\n",
    "<img src=\"LabSession3Images/wrf.png\" width=\"768\">\n",
    "\n",
    "**Of course you can also move the camera to acquire the various calibration images.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position of the origin of the WRF on the chessboard is arbitrary, and different packages define it differently.\n",
    "\n",
    "If we are using an **unambiguous chessboard**, the WRF can at least be defined univocally.\n",
    "\n",
    "For instance, OpenCV > 2.2 places it so that:\n",
    "* the **origin** lays on the **first row or column starting and ending with black squares**\n",
    "* the resulting **z axis** in a **right-handed** reference frame **points away from the camera**.\n",
    "\n",
    "We can find two possible origins in the unambiguous pattern below (red and yellow points):\n",
    "\n",
    "<img src=\"LabSession3Images/ambiguos_pattern.png\" width=\"512\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have to know the chessboard width and height**.\n",
    "\n",
    "Depending on what we choose as origin the chessboard can be specified to the software as 8(width) times 5(height) squares, or viceversa as depticted in the image below. \n",
    "\n",
    "Choosen the width and height of the pattern and given that the z axis has to form a right handed reference frame with the others, for each origin we obtain only one possibile direction of x and y axis.\n",
    "\n",
    "The **x and y axes direction** are **fixed by the chessboard definition**.\n",
    "* **x axis** lays along the **“width”** of the chessboard\n",
    "* **y axis** along the **“height”** of the chessboard\n",
    "\n",
    "<img src=\"LabSession3Images/wrf_chessboard.png\" width=\"512\">\n",
    "\n",
    "Note that, in OpenCV to define the chessboard we use **the number of internal corners**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Camera calibration approaches can be split into **two** main categories:\n",
    "\n",
    "* Those relying on a **single image** featuring **several (at least  2) planes containing a known pattern**. \n",
    "\n",
    "* Those relying on **several (at least 3) different images** of **one given planar pattern**. (We are using this kind of approach!) \n",
    "\n",
    "<img src=\"LabSession3Images/patterns.png\" height=\"256\">\n",
    "\n",
    "In practise, it is difficult to build accurate targets containing multiple planes, while an accurate planar target can be attained rather easily. \n",
    "\n",
    "Implementing a camera calibration software requires a significant effort. However, the main Computer Vision toolbox include specific functions (OpenCV, Matlab CC Toolbox, Halcon.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zhang Method for Calibration\n",
    "<img src=\"LabSession3Images/zhang.png\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing **Camera Calibration** with *OpenCV*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all let us import the usual libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring the calibration images\n",
    "\n",
    "The **minimum** number of images to calibrate a camera is __3__. A rule of thumb is to take **at least 12** pictures because it is possible that you will not be able to detect the chessboard in all of them and it will make calibration results more robusts.\n",
    "\n",
    "Moreover, try to take both pictures with **several rotations of the chessboard**.\n",
    "\n",
    "When you acquired the pictures save them on your computer in a known folder.\n",
    "\n",
    "Let us create a list of the paths to all the calibration images. (**The path to the images you have just taken if you want to calibrate your own camera** or you can use the pictures contained inside *\"chessboards\"*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dirname = \"chessboards/\"\n",
    "img_names = [dirname + str(i) + \".jpg\" for i in range(13)]\n",
    "print(img_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding 2D-3D correspondeces\n",
    "\n",
    "First of all we need to perform the first step of Zhang method. Finding all 2D-3D correspondeces between corner 2D position in the image and 3D world coordinates.\n",
    "\n",
    "<img src=\"LabSession3Images/zhang_1.png\" width=\"256\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Coordinates\n",
    "\n",
    "We need to measure the size of the square (measure it direcly using a ruler on the printed chessboard) to know the 3D coordinates of the points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "square_size = 26.5 #mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then count the number of **inner corner** per row and column of the calibration target (chessboard). We will call it *pattern_size*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pattern_size = (8,5) # number of inner corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the 3D coordinate for each corner of the chessboard. \n",
    "\n",
    "We know that 3D coordinates of a corner $c_{i,j}$ will be *i x square_size* and *j x square_size* where $i,j$ are the row and column of the corner in the pattern grid. \n",
    "\n",
    "The total number of corner is *rows x columns* of the pattern size.\n",
    "\n",
    "To build the $i,j$ coordinate for each point in the grid is really useful the numpy method *np.indices(shape)* which returns an rows x columns array containing the 2D indices of that array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indices = np.indices(pattern_size, dtype=np.float32)\n",
    "print(\"Shape of indices: \" , indices.shape)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B** we need it in **float** type because later we will need to multiply it with float numbers.\n",
    "\n",
    "Let us see for instance the x and y indices of the position 1,1 in the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(indices[:,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1,1) of course!\n",
    "\n",
    "Since we know that the distance between the corners is exactly 26.5mm (square_size), we have to multiply these indices by square_size to get the real 3D x,y coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indices *= square_size\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the x,y coordinates of each corner in the world reference system ! (assumed that the first is in position (0mm,0mm)). For instance let us try to plot again the 1,1 corner position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(indices[:,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corner $c_{1,1}$ is in position 26.5,26.5 mm in the world! Moreover, during the camera calibration we assume that all the pixels belonging to the chessboard lie in the same plane with z=0.\n",
    "\n",
    "So the 3D coordinate in the WRF of each corner is just x,y,0!\n",
    "\n",
    "Let us create an array for containing each 3D cordinate. It will have shape *rows x columns x 3*  where rows and columns are referred to the pattern size while 3 is because we need x,y,z values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pattern_points = np.zeros([pattern_size[0]*pattern_size[1], 3], np.float32)\n",
    "print(pattern_points.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us assing each x,y to pattern_points. We need to transpose the (__2__,8,5) to become a (5,8,**2**) array and then reshape it to (40,2). Then we assign this vector to the pattern_points[:,:2] getting our array of (x,y,0) 3D coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coords_3D = indices.T\n",
    "print(\"Transpose shape: \" , coords_3D.shape)\n",
    "\n",
    "coords_3D = coords_3D.reshape(-1, 2)\n",
    "print(coords_3D.shape)\n",
    "\n",
    "pattern_points[:, :2] = coords_3D\n",
    "\n",
    "# The croner [0,1] of the grid will be in posizion 1\n",
    "print(\"3D coordinates of Corner [0,1]: \",  pattern_points[1])\n",
    "\n",
    "# The croner [1,1] of the grid will be in posizion 9+1=10 where 9 is pattern_size[0] (width)\n",
    "print(\"3D coordinates of Corner [1,1]: \",  pattern_points[pattern_size[0]*1 + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Coordinates\n",
    "\n",
    "Now that we have the 3D coordinate of each corner in the world reference system we need to get the cooresponding 2D coordinate of each corner in each chessboard images. \n",
    "\n",
    "Let us first load a sample image to understand how to detect corners. It needs to be loaded **GRAYSCALE**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(\"chessboards/0.jpg\",cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **cv2.findChessboardCorners(img, pattern_size)** to detect corner in an image. \n",
    "\n",
    "The functions will return a boolean value **found** and the list of the **corners** 2D coordinate in the image. **found** will be true if and only if all the 8x5 (40) corners will be detected in the image. If the image is too dark or too bright the algorithm may fail to detect corners and **found** would be false. \n",
    "\n",
    "**N.B** If you pass the wrong pattern_size the method will be really slow leading to a **found=False** result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "found, corners = cv2.findChessboardCorners(img, pattern_size)\n",
    "print(\"Found: \" , found)\n",
    "print(\"2D image coordinate of corners: \", corners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we **found** the 2D positions of corners, they might not be accurate. \n",
    "\n",
    "To refine the results we can call **cv2.cornerSubPix**. \n",
    "\n",
    "See in OpenCV [documentation](https://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=cornersubpix#cornersubpix*documentation.) for more detail about this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Do cornerSubPix only if chessboard found is True\n",
    "if found:\n",
    "    #Refining corner position to subpixel iteratively until criteria max_count=30 or criteria_eps_error=1 is sutisfied\n",
    "    term = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_COUNT, 30, 1)\n",
    "    #Image Corners \n",
    "    cv2.cornerSubPix(img, corners, (5, 5), (-1, -1), term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the founded corners. Follow **from red to blu** lines to go from (0,0) to (8,5) corner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "cv2.drawChessboardCorners(vis, pattern_size, corners, found)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we calibrate a camera we need to repeat this operation on all calibration images so I will define a function to process images which return the pairs of (2D,3D) points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def processImage(fn):\n",
    "    print('processing {}'.format(fn))\n",
    "    img = cv2.imread(fn, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if img is None:\n",
    "        print(\"Failed to load\", fn)\n",
    "        return None\n",
    "\n",
    "    found, corners = cv2.findChessboardCorners(img, pattern_size)\n",
    "\n",
    "    if found:\n",
    "        #Refining corner position to subpixel iteratively until criteria  max_count=30 or criteria_eps_error=1 is sutisfied\n",
    "        term = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_COUNT, 5, 1)\n",
    "        #Image Corners \n",
    "        cv2.cornerSubPix(img, corners, (5, 5), (-1, -1), term)\n",
    "\n",
    "    vis = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.drawChessboardCorners(vis, pattern_size, corners, found)\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(vis)\n",
    "    plt.show()\n",
    "    \n",
    "    if not found:\n",
    "        print('chessboard not found')\n",
    "        return None\n",
    "\n",
    "    print('           %s... OK' % fn)\n",
    "    return (corners.reshape(-1, 2), pattern_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then process each chessboard image we acquired previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chessboards = [processImage(fn) for fn in img_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we filter out each not found chessboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chessboards = [x for x in chessboards if x is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add to two lists each 2D and 3D points respectively building the set of 2D-3D correspondeces used during calibration process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating the lists of 2D and 3D points \n",
    "obj_points = [] #3D points\n",
    "img_points = [] #2D points\n",
    "\n",
    "for (corners, pattern_points) in chessboards:\n",
    "        img_points.append(corners)\n",
    "        obj_points.append(pattern_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrate Camera\n",
    "\n",
    "Now that we got all 3D points (obj_points) and their corresponding 2D points (img_points) we can calibrate our camera and perform all other Zhang steps:\n",
    "    \n",
    "<img src=\"LabSession3Images/zhang_all.png\" width=\"256\">\n",
    "\n",
    "**READY ???????**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LUCKY** OpenCV provides a single method to it! (Danger Escaped! :D )\n",
    "\n",
    "**cv2.calibrateCamera(3D_points, 2D_points, (width,height), None,None)**\n",
    "\n",
    "This method takes as input the corresponding points and the width and height of the image and it returns all the parameters of the camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Getting the width and height of the images\n",
    "h, w = cv2.imread(img_names[0], cv2.IMREAD_GRAYSCALE).shape[:2]\n",
    "\n",
    "# Calibrating Camera\n",
    "rms, camera_matrix, dist_coefs, rvecs, tvecs = cv2.calibrateCamera(obj_points, img_points, (w, h), None, None)\n",
    "\n",
    "print(\"\\nRMS:\", rms)\n",
    "print(\"camera matrix:\\n\", camera_matrix)\n",
    "print(\"distortion coefficients: \", dist_coefs.ravel())\n",
    "print(\"Rotation vectors:\", rvecs)\n",
    "print(\"translation vectors\", tvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that we got several outputs from *cv2.calibrateCamera*. They are respectively:\n",
    "* RMS (Root Mean Square Error): **Reprojection error in pixel** using the estimated camera parameters. The lower it is the better calibration you obtain. Usually it should be in a range **between 0.1 and 1** in good calibrations. If it is much larger than 1 you did some mistakes during the calibration process.\n",
    "\n",
    "* camera_matrix: **Intrinsics parameters** of the camera expressed as 3x3 matrix\n",
    "\n",
    "* dist_coefs: **lens distorsion** coefficents expressed as 1x6 array.\n",
    "\n",
    "* rvecs: **rotations** of the cameras **for each chessboard image**. It is an array of dimension Nx3 where N are the number of images where you found the chessboard during calibration while 3 are the 3 DOF of the rotations.\n",
    "\n",
    "* tvecs: **translations** of the cameras **for each chessboard image**. It is an array of dimension Nx3 where N are the number of images where you found the chessboard during calibration while 3 are the coordiante of the 3D translation vectors.\n",
    "\n",
    "**N.B** If we need the rotation matrix starting from a rotation vector rvec or vice-versa we can use:\n",
    "\n",
    "**cv2.Rodrigues(rvec,tvec)**.\n",
    "\n",
    "Given a rvec it will return the 3x3 rotation matrix. Let us try with the first element of rvecs found by calibration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rotation_matrix = cv2.Rodrigues(rvecs[0])[0]\n",
    "print(\"R shape: \", rotation_matrix.shape)\n",
    "print(rotation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we need the complete 3x4 extrinsics matrix we need to concatenate the rotation matrix and the traslation vector along axis 1. To do it we can use **np.concatenate(listOfArrays, axis)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "translation_matrix = tvecs[0]\n",
    "print(\"T shape\", translation_matrix.shape)\n",
    "\n",
    "extrinsics_matrix = np.concatenate([rotation_matrix,translation_matrix], axis=1)\n",
    "print(\"RT shape: \", extrinsics_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undistorting images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the **lens distorsions** and the **intrisics parameters** we can undistort images.\n",
    "\n",
    "__Reminder__ There are two main kind of distorsion in lenses:\n",
    "\n",
    "* **Radial distortion**: lines far from the principal point look distorted Radial distortion is modeled using the following relationship between the undistorted coordinates  $x_u$  and  $y_u$  and the distorted ones $x$ and $y$:\n",
    "\n",
    "<p style=\"text-align: center\"> $x_u = x(1+k_1r_2+k_2r_4+k_3r_6)$ </p>\n",
    "<p style=\"text-align: center\"> $y_u = y(1+k_1r_2+k_2r_4+k_3r_6)$ </p>\n",
    "\n",
    "**<p style=\"text-align: center\"> Pincushion distorsion: </p>**\n",
    "<img src=\"LabSession3Images/pincushion_distortion.png\" width=\"128\">\n",
    "**<p style=\"text-align: center\"> Barrel distorsion: </p>**\n",
    "<img src=\"LabSession3Images/barrel_distortion.png\" width=\"128\">\n",
    "\n",
    "* **Tangential distortion**: occurring becouse the lens is not perfectly parallel to the camera plane.\n",
    "\n",
    "<img src=\"LabSession3Images/tangent_distortion.png\" width=\"256\">\n",
    "\n",
    "In sum, we need five parameters to model camera distortion:  $[k1,k2,k3,p1,p2]$ , which are the five values returned by the function cv2.calibrateCamera in the dist_coefs variable.\n",
    "\n",
    "To undistort an image in OpenCV we only need to call **cv2.getOptimalNewCameraMatrix** then **cv2.initUndistortRectifyMap** and finally **cv2.remap**.\n",
    "\n",
    "* **cv2.getOptimalNewCameraMatrix** : If the scaling parameter alpha=0, it returns undistorted image with minimum unwanted pixels. So it may even remove some pixels at image corners. If alpha=1, all pixels are retained with some extra black images. It also returns an image ROI which can be used to crop the result.\n",
    "* **cv2.initUndistortRectifyMap**:  Find a mapping function from distorted image to undistorted image. \n",
    "* **cv2.remap**: Apply a mapping function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Finding the new optical camera matrix\n",
    "newcameramtx, roi = cv2.getOptimalNewCameraMatrix(camera_matrix, dist_coefs, (w, h), 1, (w, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(\"chessboards/0.jpg\")\n",
    "# Getting the mapping between undistorted and distorted images\n",
    "mapx,mapy = cv2.initUndistortRectifyMap(camera_matrix,dist_coefs,None,newcameramtx,(w,h),5)\n",
    "# Apply the mapping\n",
    "im_undistorted = cv2.remap(img,mapx,mapy,cv2.INTER_LINEAR)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(im_undistorted, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then crop the image to the ROI returned by **cv2.getOptimalNewCameraMatrix** to exclude black pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, y, w_2, h_2 = roi\n",
    "im_undistorted = im_undistorted[y:y+h_2, x:x+w_2]\n",
    "\n",
    "# Plotting DISTORTED image\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "# Plotting UNDISTORTED image\n",
    "plt.imshow(cv2.cvtColor(im_undistorted, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise we can directly use **cv2.undistort** which is a simple combination of **cv2.initUndistortRectifyMap** and **cv2.remap**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "im_undistorted = cv2.undistort(img, camera_matrix, dist_coefs, None, newcameramtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And crop to the ROI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, y, w_2, h_2 = roi\n",
    "im_undistorted = im_undistorted[y:y+h_2, x:x+w_2]\n",
    "\n",
    "plt.imshow(cv2.cvtColor(im_undistorted, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of Applications: Aumegmented Reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the camera is calibrated for both the extrinsic and intrinsic parameters, we can project points from the 3D world to the 2D image plane. This can be used, for instance, to implement \"augmented reality\" algorithms which draw 3D objects on the image. Let's see how to draw a cube on the checkerboard. First, define the  8  corners of a cube of side 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_3d_corners = np.float32([[0,0,0], [0,100,0], [100,100,0], [100,0,0],\n",
    "                           [0,0,-100],[0,100,-100],[100,100,-100],[100,0,-100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can project points to the 2D image plane using the function cv2.projectPoints. Since we need to project them on a specific image, we first need to choose one of the considered images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_index=0\n",
    "cube_corners_2d,_ = cv2.projectPoints(_3d_corners,rvecs[image_index],tvecs[image_index],camera_matrix,dist_coefs) \n",
    "#the underscore allows to discard the second output parameter (see doc)\n",
    "\n",
    "print(cube_corners_2d,0) #the output consists in 8 2-dimensional points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot limes on the 3D image using the cv2.line function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img=cv2.imread(img_names[image_index]) #load the correct image\n",
    "\n",
    "red=(0,0,255) #red (in BGR)\n",
    "blue=(255,0,0) #blue (in BGR)\n",
    "green=(0,255,0) #green (in BGR)\n",
    "line_width=50\n",
    "\n",
    "#first draw the base in red\n",
    "cv2.line(img, tuple(cube_corners_2d[0][0]), tuple(cube_corners_2d[1][0]),red,line_width)\n",
    "cv2.line(img, tuple(cube_corners_2d[1][0]), tuple(cube_corners_2d[2][0]),red,line_width)\n",
    "cv2.line(img, tuple(cube_corners_2d[2][0]), tuple(cube_corners_2d[3][0]),red,line_width)\n",
    "cv2.line(img, tuple(cube_corners_2d[3][0]), tuple(cube_corners_2d[0][0]),red,line_width)\n",
    "\n",
    "#now draw the pillars\n",
    "cv2.line(img, tuple(cube_corners_2d[0][0]), tuple(cube_corners_2d[4][0]),blue,line_width)\n",
    "cv2.line(img, tuple(cube_corners_2d[1][0]), tuple(cube_corners_2d[5][0]),blue,line_width)\n",
    "cv2.line(img, tuple(cube_corners_2d[2][0]), tuple(cube_corners_2d[6][0]),blue,line_width)\n",
    "cv2.line(img, tuple(cube_corners_2d[3][0]), tuple(cube_corners_2d[7][0]),blue,line_width)\n",
    "\n",
    "#finally draw the top\n",
    "cv2.line(img, tuple(cube_corners_2d[4][0]), tuple(cube_corners_2d[5][0]),green,line_width)\n",
    "cv2.line(img, tuple(cube_corners_2d[5][0]), tuple(cube_corners_2d[6][0]),green,line_width)\n",
    "cv2.line(img, tuple(cube_corners_2d[6][0]), tuple(cube_corners_2d[7][0]),green,line_width)\n",
    "cv2.line(img, tuple(cube_corners_2d[7][0]), tuple(cube_corners_2d[4][0]),green,line_width)\n",
    "    \n",
    "plt.imshow(img[...,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P as Homography\n",
    "\n",
    "Due to the choice of the WRF associated with calibration images, in each of them we consider only 3D points with z=0. Accordingly, the PPM boils down to a simpler transformation defined by a 3x3 matrix:\n",
    "\n",
    "<img src=\"LabSession3Images/p_as_homography.png\" width=\"512\">\n",
    "\n",
    "* Such a transformation, denote here as $H$, is known as homography and represents a general linear transformation between planes. Above, $w’$ represents vector $(x; y; 1)$. $H$ can be thought of as a simplification of $P$ in case the imaged object is planar.\n",
    "* Given a pattern with m corner, we can write m systems of 3 linear equations as above, wherein both 3D as well as 2D coordinates are known due to corners having been detected in the i th image and the unknowns are thus the 9 elements in $H_i$. However, as $H_i$ , and $P_i$ alike, is known up to an arbitrary scale factor, the independent elements in $H_i$ are indeed just 8.\n",
    "\n",
    "Given a set of corresponding point we may want to find directly find the homography between them. We can do it with\n",
    "\n",
    "**cv2.findHomography(source_points, destination_points, method)**\n",
    "\n",
    "Where:\n",
    "* source_points = set of source points \n",
    "* destination_points = set with the corresponding points (same order as source_points)\n",
    "* method = most used  are method=0 for least square optimazation or 1 for RANSAC.\n",
    "\n",
    "Otherwise if we have **only 4 points** we can also use:\n",
    "\n",
    "**cv2.getPerspectiveTransform(source_points, destination_points)**\n",
    "\n",
    "which calculate the transformation in a closed form using the minimum number of points (8 variable, 4 points with 8 total coordinates). It requires the point in [:,2] shape ignoring the depth = 0 channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corners_0, pattern_points_0 = processImage(img_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homography = cv2.findHomography(pattern_points_0[:,:2], corners_0)[0]\n",
    "print(homography)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "points3D_4 = pattern_points_0[[0,10,30, 39],:2].astype(np.float32)\n",
    "points2D_4 = corners_0[[0,10,30, 39],:2].astype(np.float32)\n",
    "\n",
    "pt = cv2.getPerspectiveTransform(points3D_4,points2D_4)\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warping\n",
    "    \n",
    "At we did for undistorsion sometimes we know a mapping (e.g homography) between points (e.g. two images) and we would like to warp an image over another.\n",
    "\n",
    "To apply a perspective transformation we can use:\n",
    "\n",
    "**cv2.WarpPerspective(src_image, tranformation_matrix, destination_shape)**\n",
    "\n",
    "If we want to perform the inverse trasformation it is useful **np.linalg.inv(matrix)** which calculates the inverse of a matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
