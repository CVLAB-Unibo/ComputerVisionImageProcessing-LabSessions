{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with Convolutional Neural Networks in Keras\n",
    "## Computer Vision and Image Processing - Lab Session 6 Excercises Solutions\n",
    "### Prof: Luigi Di Stefano, luigi.distefano@unibo.it\n",
    "### Tutor: Pierluigi Zama Ramirez, pierluigi.zama@unibo.it - Alex Costanzino, alex.costanzino@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Data normalization\n",
    "\n",
    "Try to reimplement the code explained in the theory. Load the data, inspect it and train a new classifier without performing data normalization, keeping the same parameters of the original experiment.\n",
    "* Are there any differences in terms of performances?\n",
    "* How does normalization affect the training time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "422/422 [==============================] - 45s 105ms/step - loss: 0.8853 - accuracy: 0.7856 - val_loss: 0.4208 - val_accuracy: 0.8472\n",
      "Epoch 2/5\n",
      "422/422 [==============================] - 45s 107ms/step - loss: 0.4254 - accuracy: 0.8490 - val_loss: 0.3788 - val_accuracy: 0.8650\n",
      "Epoch 3/5\n",
      "422/422 [==============================] - 45s 107ms/step - loss: 0.3797 - accuracy: 0.8658 - val_loss: 0.3574 - val_accuracy: 0.8743\n",
      "Epoch 4/5\n",
      "422/422 [==============================] - 45s 108ms/step - loss: 0.3515 - accuracy: 0.8757 - val_loss: 0.3429 - val_accuracy: 0.8788\n",
      "Epoch 5/5\n",
      "422/422 [==============================] - 45s 108ms/step - loss: 0.3273 - accuracy: 0.8827 - val_loss: 0.3228 - val_accuracy: 0.8852\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3466 - accuracy: 0.8753\n",
      "The test loss is 0.3466, the test accuracy is 0.8753.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixed random seed for repeatability.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "def run_experiment(x_train, y_train, x_test, y_test,\n",
    "                   batch_size, epochs, val_split_percentage,\n",
    "                   filters, rate, lr):\n",
    "\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "    input_shape = x_train.shape[1:]\n",
    "    n_classes = len(np.unique(y_train))\n",
    "\n",
    "    model = Sequential(\n",
    "        [   \n",
    "            # Input layer.\n",
    "            Input(shape = input_shape),\n",
    "\n",
    "            # Convolutions with subsequent pooling.\n",
    "            Conv2D(filters = filters, kernel_size = (3, 3), activation = \"relu\"),\n",
    "            MaxPooling2D(pool_size = (2, 2)),\n",
    "            Conv2D(filters = filters**2, kernel_size = (3, 3), activation = \"relu\"),\n",
    "            MaxPooling2D(pool_size = (2, 2)),\n",
    "\n",
    "            # Classification head.\n",
    "            Flatten(),\n",
    "            Dropout(rate = rate),\n",
    "            Dense(units = n_classes, activation = \"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    opt = SGD(learning_rate = lr)\n",
    "    loss_fcn = SparseCategoricalCrossentropy()\n",
    "\n",
    "    model.compile(loss = loss_fcn, \n",
    "                optimizer = opt, \n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_split = val_split_percentage)\n",
    "\n",
    "    test_loss, test_metric = model.evaluate(x_test, y_test, verbose = 1)\n",
    "    print(f\"The test loss is {test_loss:.4f}, the test accuracy is {test_metric:.4f}.\")\n",
    "\n",
    "run_experiment(x_train, y_train, x_test, y_test,\n",
    "               batch_size = 128, epochs = 5, val_split_percentage = 0.1,\n",
    "               filters = 32, rate = 0.15, lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Repeat the experiment with the MNIST dataset\n",
    "\n",
    "Try to reimplement the code explained in the theory with the [MNIST dataset](https://keras.io/api/datasets/mnist/). Load the data, inspect it and train a new classifier, keeping the same parameters of the original experiment.\n",
    "* Are there any differences in terms of qualitative performances (i.e. accuracy, loss)?\n",
    "* Are there any differences in terms of temporal performances?\n",
    "* If there are differences what may be the source and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the data if you are using your own device:\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixed random seed for repeatability.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data();\n",
    "\n",
    "def run_experiment(x_train, y_train, x_test, y_test,\n",
    "                   batch_size, epochs, val_split_percentage,\n",
    "                   filters, rate, lr):\n",
    "\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "    input_shape = x_train.shape[1:]\n",
    "    n_classes = len(np.unique(y_train))\n",
    "\n",
    "    model = Sequential(\n",
    "        [   \n",
    "            # Input layer.\n",
    "            Input(shape = input_shape),\n",
    "\n",
    "            # Convolutions with subsequent pooling.\n",
    "            Conv2D(filters = filters, kernel_size = (3, 3), activation = \"relu\"),\n",
    "            MaxPooling2D(pool_size = (2, 2)),\n",
    "            Conv2D(filters = filters**2, kernel_size = (3, 3), activation = \"relu\"),\n",
    "            MaxPooling2D(pool_size = (2, 2)),\n",
    "\n",
    "            # Classification head.\n",
    "            Flatten(),\n",
    "            Dropout(rate = rate),\n",
    "            Dense(units = n_classes, activation = \"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    opt = SGD(learning_rate = lr)\n",
    "    loss_fcn = SparseCategoricalCrossentropy()\n",
    "\n",
    "    model.compile(loss = loss_fcn, \n",
    "                optimizer = opt, \n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_split = val_split_percentage)\n",
    "\n",
    "    test_loss, test_metric = model.evaluate(x_test, y_test, verbose = 1)\n",
    "    print(f\"The test loss is {test_loss:.4f}, the test accuracy is {test_metric:.4f}.\")\n",
    "\n",
    "run_experiment(x_train, y_train, x_test, y_test,\n",
    "               batch_size = 128, epochs = 5, val_split_percentage = 0.1,\n",
    "               filters = 32, rate = 0.15, lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Play with the parameters\n",
    "\n",
    "Try to reimplement the code explained in the theory. Load the data, inspect it and train a new classifier changing several parameters (i.e. learning rate, optimizers, number of filters, kernel size, batch size, epochs, etc...), one at a time. \n",
    "\n",
    "Keep track of each parameter change and the corresponding change in model performance.\n",
    "* How each parameter change affects the model performance? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixed random seed for repeatability.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "def run_experiment(x_train, y_train, x_test, y_test,\n",
    "                   batch_size, epochs, val_split_percentage,\n",
    "                   filters, rate, lr):\n",
    "\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "    input_shape = x_train.shape[1:]\n",
    "    n_classes = len(np.unique(y_train))\n",
    "\n",
    "    model = Sequential(\n",
    "        [   \n",
    "            # Input layer.\n",
    "            Input(shape = input_shape),\n",
    "\n",
    "            # Convolutions with subsequent pooling.\n",
    "            Conv2D(filters = filters, kernel_size = (3, 3), activation = \"relu\"),\n",
    "            MaxPooling2D(pool_size = (2, 2)),\n",
    "            Conv2D(filters = filters**2, kernel_size = (3, 3), activation = \"relu\"),\n",
    "            MaxPooling2D(pool_size = (2, 2)),\n",
    "\n",
    "            # Classification head.\n",
    "            Flatten(),\n",
    "            Dropout(rate = rate),\n",
    "            Dense(units = n_classes, activation = \"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    opt = SGD(learning_rate = lr)\n",
    "    loss_fcn = SparseCategoricalCrossentropy()\n",
    "\n",
    "    model.compile(loss = loss_fcn, \n",
    "                optimizer = opt, \n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_split = val_split_percentage)\n",
    "\n",
    "    test_loss, test_metric = model.evaluate(x_test, y_test, verbose = 1)\n",
    "    print(f\"The test loss is {test_loss:.4f}, the test accuracy is {test_metric:.4f}.\")\n",
    "\n",
    "\n",
    "run_experiment(x_train, y_train, x_test, y_test,\n",
    "               batch_size = 64, epochs = 5, val_split_percentage = 0.1,\n",
    "               filters = 32, rate = 0.15, lr = 1e-3)\n",
    "\n",
    "run_experiment(x_train, y_train, x_test, y_test,\n",
    "               batch_size = 128, epochs = 5, val_split_percentage = 0.1,\n",
    "               filters = 16, rate = 0.15, lr = 1e-3)\n",
    "\n",
    "run_experiment(x_train, y_train, x_test, y_test,\n",
    "               batch_size = 128, epochs = 5, val_split_percentage = 0.1,\n",
    "               filters = 32, rate = 0.15, lr = 1e-5)\n",
    "\n",
    "run_experiment(x_train, y_train, x_test, y_test,\n",
    "               batch_size = 128, epochs = 15, val_split_percentage = 0.1,\n",
    "               filters = 32, rate = 0.15, lr = 1e-3)    \n",
    "\n",
    "# And so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Play with the model's architecture\n",
    "\n",
    "Try to reimplement the code explained in the theory. Load the data, inspect it and train a new classifier changing the model's architecture (i.e. add or remove a convolutional layer, add more dense layers, etc...), one at a time. \n",
    "\n",
    "Keep track of each model change and the corresponding change in model performance.\n",
    "* How each model change affects the model performance? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixed random seed for repeatability.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "def run_experiment(x_train, y_train, x_test, y_test,\n",
    "                   batch_size, epochs, val_split_percentage,\n",
    "                   filters, rate, lr):\n",
    "\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "    input_shape = x_train.shape[1:]\n",
    "    n_classes = len(np.unique(y_train))\n",
    "\n",
    "    model = Sequential(\n",
    "        [   \n",
    "            # Input layer.\n",
    "            Input(shape = input_shape),\n",
    "\n",
    "            # Convolutions with subsequent pooling.\n",
    "            Conv2D(filters = filters, kernel_size = (3, 3), activation = \"relu\"),\n",
    "            MaxPooling2D(pool_size = (2, 2)),\n",
    "\n",
    "            # Classification head.\n",
    "            Flatten(),\n",
    "            Dropout(rate = rate),\n",
    "            Dense(units = n_classes, activation = \"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    opt = SGD(learning_rate = lr)\n",
    "    loss_fcn = SparseCategoricalCrossentropy()\n",
    "\n",
    "    model.compile(loss = loss_fcn, \n",
    "                optimizer = opt, \n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_split = val_split_percentage)\n",
    "\n",
    "    test_loss, test_metric = model.evaluate(x_test, y_test, verbose = 1)\n",
    "    print(f\"The test loss is {test_loss:.4f}, the test accuracy is {test_metric:.4f}.\")\n",
    "\n",
    "run_experiment(x_train, y_train, x_test, y_test,\n",
    "               batch_size = 128, epochs = 20, val_split_percentage = 0.1,\n",
    "               filters = 32, rate = 0.15, lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixed random seed for repeatability.\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "def run_experiment(x_train, y_train, x_test, y_test,\n",
    "                   batch_size, epochs, val_split_percentage,\n",
    "                   filters, rate, lr):\n",
    "\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "    input_shape = x_train.shape[1:]\n",
    "    n_classes = len(np.unique(y_train))\n",
    "\n",
    "    model = Sequential(\n",
    "        [   \n",
    "            # Input layer.\n",
    "            Input(shape = input_shape),\n",
    "\n",
    "            # Convolutions with subsequent pooling.\n",
    "            Conv2D(filters = filters, kernel_size = (3, 3), activation = \"relu\"),\n",
    "            MaxPooling2D(pool_size = (2, 2)),\n",
    "            Conv2D(filters = filters**2, kernel_size = (3, 3), activation = \"relu\"),\n",
    "            MaxPooling2D(pool_size = (2, 2)),\n",
    "\n",
    "            # Classification head.\n",
    "            Flatten(),\n",
    "            Dropout(rate = rate),\n",
    "            Dense(units = n_classes*2, activation = \"softmax\"),\n",
    "            Dense(units = n_classes, activation = \"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    opt = SGD(learning_rate = lr)\n",
    "    loss_fcn = SparseCategoricalCrossentropy()\n",
    "\n",
    "    model.compile(loss = loss_fcn, \n",
    "                optimizer = opt, \n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_split = val_split_percentage)\n",
    "\n",
    "    test_loss, test_metric = model.evaluate(x_test, y_test, verbose = 1)\n",
    "    print(f\"The test loss is {test_loss:.4f}, the test accuracy is {test_metric:.4f}.\")\n",
    "\n",
    "run_experiment(x_train, y_train, x_test, y_test,\n",
    "               batch_size = 128, epochs = 20, val_split_percentage = 0.1,\n",
    "               filters = 32, rate = 0.15, lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 [at home]: Desing a classifier for the CIFAR10 dataset\n",
    "\n",
    "Desing a novel classifier with the [CIFAR10 dataset](https://keras.io/api/datasets/cifar10/). Load the data, inspect it, desing and train a new classifier guiding your desing's choices with the results of the previous experiments.\n",
    "\n",
    "*Note*: keep in mind that this is a RGB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the data\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 [at home]: Fine-tune a ResNet on a custom dataset\n",
    "\n",
    "Create a simple custom dataset with the procedure described in the theory.\n",
    "Then, load a pre-trained ResNet and test it on the custom dataset.\n",
    "Next, fine-tune the pre-trained ResNet on the custom dataset and then assess the quality of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
